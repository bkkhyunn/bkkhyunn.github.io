---
title: "[CNN] 1. Convolutional Neural Network (2)"
excerpt: "CNN 에 대해 알아보자"

categories: "cnn"
tags:
    - deep learning
    - computer vision
    - CNN
toc: true  
toc_sticky: true
toc_label: "Contents In Page"
author_profile: true
use_math: true

date: 2024-04-10
---

[Computer Vision Applications]
    
- Semantic Segmentation and Detection
- 앞에서 배웠던 것들이 이미지 분류에만 사용되진 않음. semantic segmentation 이나 detection 에도 사용됨.
- Semantic Segmentation
- 이 문제가 어떤 문제일지 기본적인 세팅을 보자.

![Untitled](/assets/images/DL_basic/Untitled%2054.png)

- 어떤 이미지가 있을 때 **각 이미지의 픽셀마다 ‘분류’를 하는 것**. 이미지 한 장이 있을 때 그 한 장을 분류하는 게 일반적인데, 그 이미지의 모든 픽셀이 각각 어떤 라벨에 속하는지를 각각 보고싶은 것.
    
    → **이미지 분류는 전체 이미지에 대한 한 가지 레이블을 할당하는 반면, Semantic Segmentation은 각 픽셀에 레이블을 할당함.**
    
- semantic segmentation 은 dense(per pixel) classification 이라 불리기도 함.
- 어디에 활용될까? 자율주행이나 운전보조장치 등에 활용됨.
- 지금까지 배운 기본적인 CNN 구조는 마지막에 flat으로 쭉 펴서 벡터를 만든 다음 Dense layer(fully connected layer)에 통과시켜서 label 수(내가 원하는 아웃풋 수) 만큼 아웃풋을 만드는 게 목적이었음.
    
    ![Untitled](/assets/images/DL_basic/Untitled%2055.png)
    
- 제일 기본이 되는 테크닉은 “**fully convolutional network**”
    
    ![Untitled](/assets/images/DL_basic/Untitled%2056.png)
    
    - Dense layer 를 없애고 싶은 것. conv layer 로 바꾸는 것. dense layer 를 없애는 과정을 convolutionalization. 이것에 대한 가장 큰 장점은 dense layer 가 없어지는 것. 궁극적으로 인풋과 아웃풋 출력으로만 봤을 때는 똑같음. 이미지에 conv 를 2층 거치고 flat(reshape)을 거치고 dense layer 를 거치는 것이나, conv 을 거치는 것은 파라미터가 정확히 일치함. 왜냐면 아무것도 바꾼 게 없음. 단지 fc layer 를 conv 로 바꿔준 것.
    - 즉, 20 x 20 x 100 을 40만짜리 벡터로 바꿔서 dense 를 하는 게 아니라, 20 x 20 x 100 짜리 필터를 만들어서 그 커널(필터)로 1x1x1000짜리 conv feature map 을 만드는 것. 그러면 여기 필요한 파라미터 숫자는 역시나 dense 와 똑같음.
        
        ![Untitled](/assets/images/DL_basic/Untitled%2057.png)
        
    - conv 연산에서 숨어있는 것은 각각의 커널의 채널, depth-wise dimension 은 내가 conv 연산을 하는 conv feature map(인풋) 의 채널과 동일함.
    - fc layer 나 conv layer 나 똑같은 연산이라는 것. 이런 것을 convolutionalization 이라 함.
    - 그러면 파라미터 수 똑같고 네트워크 똑같고, 아웃풋도 똑같은데 왜 이렇게 할까? 특히 semantic segmentation 관점에서?
    - **fully-convolutional Network 가 가지는 가장 큰 특징은, 인풋의 special dimension 에 independent 하다는 것**
        
        ⇒ transforming fully connected layers(fc-layer) into convolution layers enables a classification net to output a heat map
        
    - 어떤 네트워크에서 아웃풋이 하나의 classification label 이었다면, 더 큰 이미지에 대해서도 원래 네트워크에서는 reshape(flat)이 있어서 할 수 없었지만 fully conv net 은 인풋 이미지에 상관없이 네트워크가 돌아감. 그리고 아웃풋이 커지면 그것에 비례해서 뒷단의 special dimension 이 커지게 됨. 왜 그럴 수 있냐면 conv 이 가지는 shared parameter 성질 때문. conv 은 인풋 이미지가 special 하게 즉 width, height 가 커지는 것에 상관없이 동일한 conv 필터가 동일하게 찍히기 때문에 그게 찍어져서 나오는 result special dimension 만 같이 커지지 여전히 동작 시킬 수 있는 것. 그리고 그 동작이 마치 heat map 처럼 효과가 있는 것. 그래서 해당 이미지에 고양이가 어디에 있는지 히트맵이 나오게 됨.
        
        ![Untitled](/assets/images/DL_basic/Untitled%2058.png)
        
    - 여기서 주의할 점은 special dimension 이 많이 줄어들었다는 것. 그러나 얻을 수 있는 것은 이미지가 커지면 그것에 대해서 fully conv net 를 하게 되면 단순히 분류만 했던 네트워크가 히트맵(러프, corase하지만)이 나올 수 있는 가능성이 생긴 것.
        
        → **또 주의할 점은 FCN 은 고정된 크기의 출력이 나오지만, 픽셀 수준의 분류를 제공한다.**
        
    - 그래서 **FCN(fully convolutional network) 은 어떠한 인풋 사이즈(special dim)도 돌아갈 수 있지만**, output special dim 이것 역시 줄어들기는 함. 그래서 이런 coarse output 혹은 special resolution(해상도)가 떨어져 있는 아웃풋을 원래의 dense pixel 로 바꿔줘야 함. 즉 늘리는 역할이 필요함. 이 늘리는 방법으로써, deconvolution, unpooling 이런 논문들이 나옴.
    - Deconvolution(conv transpose) → conv 역연산
        
        ![Untitled](/assets/images/DL_basic/Untitled%2059.png)
        
        - conv 를 하고 stride 를 2 주면 special dim이 대략 반 정도 줄게 됨. 그러면 deconv 는 stride를 2를 주면 2배 늘리게 됨. deconv 는 special dim 을 키워주는 것.
        - 사실 conv의 역연산은 존재할 수 없음. 왜냐면 conv 연산은 3x3 의 정보를 합쳐서 하나의 픽셀로 만들기 때문에 이것을 역으로 복원하는 것은 불가능 함. 어떤 것을 더해서 하나의 값이 나오는데 이 구성 요소를 맞추기는 어려움. 즉 엄밀히 말하면 deconv 가 conv 의 역은 아님. 그러나 이렇게 이해하면 좋은 이유는, 네트워크를 만들 때 파라미터의 숫자와 네트워크 아키텍쳐의 크기를 계산할 때 역이라고 생각하면 계산하기 편해짐.
            
            ![Untitled](/assets/images/DL_basic/Untitled%2060.png)
            
        - deconv 연산은 패딩을 엄청 줘서 우리가 원하는 결과를 나오게 만드는 것. deconv 는 conv 의 엄밀한 역은 아니지만, 파라미터의 숫자와 네트워크의 입력과 출력의 느낌으로 봤을 때는 같다는 것.
        - **Deconvolution은 실제로 이미지 복원에 사용되는 전통적인 방법을 나타내지 않으며, 컴퓨터 비전 및 딥러닝 컨텍스트에서의 'deconvolution'은 주로 업샘플링 및 해상도 복구에 사용되는 연산을 의미한다.**
        - 인풋이미지가 들어가고 FCN 을 통과해서 per pixel classification 이 구조가 됨.
        
        ![Untitled](/assets/images/DL_basic/Untitled%2061.png)
        
- Detection
- 이미지 안에서 어느 물체가 어디에 있는지. 단 per pixel 이 아닌 bounding box 를 찾는 것.
- 가장 간단한 방법이 R-CNN. 이미지 안에서 patch 를 엄청 뽑는 것(알고리즘 - selective search).

![Untitled](/assets/images/DL_basic/Untitled%2062.png)

- 이미지 안에서 몇 개의 region 을 뽑음. 다 크기가 다름. 뽑을 때는 모르고 뽑음. 여기 이미지가 있을 것 같다는 것. 그 다음 똑같은 크기로 맞춤. CNN 에 들어가려면 똑같은 크기로 맞춰야 하기 때문. 여기에 SVM 으로 분류를 함. region에 대한 conv feature 를 뽑을 때는 AlexNet
- 이미지 마다 conv feature map 을 뽑기 위해서 AlexNet 을 2000개 돌려야 해서 오래 걸림. 그리고 또 각각 분류해야 함.
- 물론 bounding regression 이라고 해서 바운딩박스를 어떻게 옮겨줘야 더 좋은지도 들어가지만 여기선 다루지 않음.
- 이미지 안에서 어느 위치에 어느 물체가 있는지 나오게 됨. 그러나 정확하지는 않음. 이런 식으로 detection 문제를 풀고자 함.
- R-CNN 의 가장 큰 문제는 이미지 안에서 바운딩 박스를 2000개 뽑으면, 2000개의 이미지 혹은 패치를 CNN 을 다 통과시켜야 하는 것. 그래서 하나의 이미지에 CNN 이 2000번 돌아가는 것
- 그래서 SPPNet 은 이미지 안에서 CNN 을 1번만 돌리자!
    
    ![아래가 SPPNet. conv layer 를 한번만!](/assets/images/DL_basic/Untitled%2063.png)
    
    아래가 SPPNet. conv layer 를 한번만!
    
    - spp 는 spatial pyramid pooling
        
        → 이렇게 불리는 이유는 뜯어온 피쳐맵을 어떻게든 잘 활용해서 하나의 fix dimension 을 벡터로 바꿔주는 데 있음. 그게 바로 spatial pyramid pooling.
        
    - 이미지 안에서 바운딩 박스를 뽑고, 이미지 전체에 대해서 CNN 만든 다음에, 뽑힌 바운딩 박스 위치에 해당하는 conv feature map 의 텐서만 들고 오자는 것. 그래서 CNN 은 한 번 돌지만 conv feature map 에 해당하는 위치의 sub-텐서를 뜯어오는 것만 region 별로 하기 때문에 R-CNN 에 비해 빨라짐.
    - SPPNet 이 가지는 가장 큰 컨셉은 CNN 을 한 번 돌려서 얻어지는 conv feature map 위에서 얻어진 바운딩 박스의 패치를 뜯어오는 것.
- Fast R-CNN
    - conv 를 한 번 돌리지만, 바운딩 박스에 해당하는 텐서를 여러개 뜯어와서 spp 로 하나의 벡터로 만들고 분류를 해줘야 하기 때문에 그래도 느림.
    - Fast R-CNN 도 SPPNet 과 동일한 컨셉을 가져옴.
    
    ![Untitled](/assets/images/DL_basic/Untitled%2064.png)
    
    - R-CNN, SPPNet 컨셉을 다 이용함.
    - 마지막에 뉴럴 네트워크를 통해서 바운딩 박스 regressor; 얻어지는 바운딩 박스를 어떻게 움직이면 좋을지, 그리고 그 바운딩 박스에 대한 label 을 찾게 됨(softmax)
    - 기본 컨셉은 SPPNet 과 비슷한데, 뒷단에 ROI(Region of Interest) feature vector를 가지고 뉴럴 네트워크를 통해서 bbox regressor 와 classification 을 했다는 것이 큰 의미.
- Faster R-CNN
    - R-CNN 시리즈의 마지막
    - 이미지를 통해서 바운딩박스를 뽑아내는 Region Proposal 도 학습을 시키자!
    - 바운딩박스를 뽑아내는 selective search 는 알고리즘일 뿐, detection 에 맞지 않음.
    - 따라서 Faster R-CNN 은 내가 바운딩박스(candidate)를 뽑는 것도 네트워크로 학습하자! → RPN(region proposal network)
    - 즉 Fast R-CNN 에 RPN 이 더해진 것.
    
    ![Untitled](/assets/images/DL_basic/Untitled%2065.png)
    
    - RPN 이란?
        - 이미지가 있으면 이 이미지에서 특정 영역(패치)이 바운딩박스로서 의미가 있을지 없을지. 이 안에 물체가 있을지 아닌지를 찾아주는 것. 이 물체가 무엇인지는 뒷단의 네트워크가 판단. RPN 이 하는 것은 이 안에 물체가 있을 것 같다! region proposal 을 해주는 것.
        - 이를 위해 anchor box 가 필요함. 이것은 미리 정해놓은 바운딩 박스의 크기. 내가 이 이미지 안에 어떤 크기의 물체들이 있을 지 알고 있는 것. 그래서 k 개의 템플릿(anchor box)들을 만들어 놓는 것.
        
        ![Untitled](/assets/images/DL_basic/Untitled%2066.png)
        
        - 이 템플릿들이 얼마나 바뀔지에 대한 offset 을 찾고 궁극적으로는 이 템플릿을 미리 고정해 두는 것이 RPN의 가장 큰 특징.
        - 여기도 Fully Conv 가 활용됨. → Fully Conv Net 이 해주는 것은 Conv feature map 의 모든 영역을 돌아가면서 찍는 것. 이것은 해당하는 영역의 이미지가 과연 어떤 물체가 들어있을지를 이 커널, Fully Conv Net 이 들고 있게 되는 것.
        
        ![Untitled](/assets/images/DL_basic/Untitled%2067.png)
        
        - RPN 의 경우 anchor box(predefined region size(3) & ratios(3)) 가 9개가 있음(combination). 그 중 하나를 정하는 것.
        - 각각의 region size 마다 바운딩 박스를 얼마나 키울지 줄일지 4개의 파라미터가 필요함. (x, y, w, h)
        - 해당 바운딩 박스가 쓸모 있는지 없는지 box classification. 내가 상대적으로 적은 숫자의 bbox proposal 을 만들어 줘야 하기 때문에 use it or not.
        - 그래서 Fully Conv 의 채널 숫자가 9(anchor box type) * (4+2) 해서 총 54개의 채널이 나오는 conv feature map 이 됨. 이 54개의 값을 뜯어보면 해당 영역에서 어떤 bbox를 사용할지 말지를 정하게 됨. (confidence)
            
            → 위에서 4+2 는 각각의 바운딩 박스에 귀속된 것이기 때문.
            
        - 그래서 그렇게 나온 RPN 뒷단에 이 region 이 어떤 클래스에 속하는지를 맞추는 식으로 detection 하는 게 Faster R-CNN
- YOLO
    - V1. Faster R-CNN 보다 훨씬 빠름. 그 이유는 RPN 를 통해서 뭔가 나와서 그 Region 에 해당하는 conv feature map 의 sub-tensor 를 가지고 분류하는 것이 아니라,
    - 그냥 이미지 한 장에서 바로 찍어서 아웃풋이 나올 수 있기 때문.
    - 제일 중요한 컨셉은, Faster R-CNN 은 바운딩 박스를 찾는 RPN 이 있었고 거기서 나오는 바운딩 박스를 따로 분류했었음. 그러나 YOLO 에서는 한번에 분류함. 바운딩박스를 따로 뽑는 RPN 스텝이 없고 한 번에 하기 때문에 속도가 빠른 것.
    
    ![Untitled](/assets/images/DL_basic/Untitled%2068.png)
    
    - 이미지가 들어오면 이미지를 S x S grid 로 나눔. 이미지 안에 내가 찾고 싶은 물체의 중앙이 해당 그리드 안에 들어가면, 그 그리드 셀이 해당 물체에 대한 바운딩 박스와 해당 물체가 무엇인지를 같이 예측 해줘야 함.
    - 먼저 바운딩 박스를 찾아줘야 함. 각각의 셀은 B 개의 바운딩 박스를 예측하게 됨. RPN 처럼 predefined 된 anchor box 가 있지는 않음. 여기서는 B개의 바운딩 박스의 x, y, w, h 를 찾고 그 바운딩 박스가 쓸모 있는지 없는지를 판단함.
    - 그와 동시에 각각의 그리드가 이 그리드 셀의 중점에 있는 어떤 오브젝트가 어떤 클래스 인지를 예측하게 됨. 원래라면 바운딩 박스를 찾고 거기서 나온 것을 따로 네트워크에 돌려서 클래스를 찾았다면 얘는 두 개가 동시에 진행되는 것.
    - 이 두개의 정보를 취합하게 되면 바운딩 박스와 그 박스가 어떤 클래스에 해당하는지가 나옴.
    - 이것을 텐서로 표현하면, 아래와 같이 됨.
    
    ![Untitled](/assets/images/DL_basic/Untitled%2069.png)
    
    - 위에서 채널(B*5+C)을 주의!
    - 각각의 채널에 맞는 정보가 끼워 들어갈 수 있도록 네트워크가 학습을 시키는 것.
    - Faster R-CNN 에 비해서 바운딩 박스를 찾아내는 것과 동시에 클래스를 찾아내는 것을 같이 하기 때문에 엄청 빠른 속도를 얻음.