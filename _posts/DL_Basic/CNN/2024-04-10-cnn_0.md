---
title: "[CNN] 1. Convolutional Neural Network (1)"
excerpt: "CNN 에 대해 알아보자"

categories: "cnn"
tags:
    - deep learning
    - computer vision
    - CNN
toc: true  
toc_sticky: true
toc_label: "Contents In Page"
author_profile: true
use_math: true

date: 2024-04-10
---

## **CNN(Convolutional Neural Network)**

### 등장배경
- 

### Convolution
- 컨볼루션은 신호 처리에서 사용되던 개념이다. 두 개의 함수를 잘 섞어주는 방법, 즉 합성곱으로 정의된다.
- 신호처리에서 시스템의 출력을 구할 때 쓰는 하나의 연산인데, 두 가지 신호가 겹쳐지는 부분의 넓이(적분)로 구해진다.
- 
- 컨볼루션은 신호 처리에서 사용되던 개념. 두 개의 함수를 잘 섞어주는 방법, operator 로 정의된 것.

![Untitled](/assets/images/DL_basic/Untitled%2026.png)

→ 이미지에서 보면, $I$는 전체 이미지 공간, $K$는 우리가 적용하고자 하는 필터의 모양

- 컨볼루션 연산

![Untitled](/assets/images/DL_basic/Untitled%2027.png)

- 컨볼루션 연산의 의미? 우리가 적용하고자 하는 필터의 모양에 따라서 같은 이미지에 대해서 convolution output 이 달라짐. 블러, 강조, 외곽선 등등
- feature 의 channel 수는 어떻게 나올까? 컨볼루션 필터가 여러개 있다고 볼 수 있는 것. 컨볼루션 필터가 1개 있으면 아웃풋의 채널은 1. 4개가 있으면 채널은 4

→ 그래서 우리가 인풋 채널과 아웃풋(컨볼루션 피쳐맵)의 채널을 알면, 여기에 적용되는 컨볼루션 피쳐의 크기 역시 계산할 수 있음.

→ RGB 이미지는 3개의 채널(Red, Green, Blue)을 가지므로, 이 이미지에 적용되는 컨볼루션 필터도 3개의 채널을 가져야 한다. 이렇게 하여 각 필터의 채널이 입력 이미지의 해당 채널과 연산된다. 그 후 모든 채널의 결과가 합쳐져 최종 출력 특성 맵(feature map)을 생성한다.

→ 컨볼루션 레이어에서 필터(또는 커널)의 개수는 출력 데이터의 채널 수를 결정합니다. 각 필터는 입력 데이터에 독립적으로 적용되며, 각 필터의 출력은 출력 데이터의 하나의 채널을 형성한다.

![Untitled](/assets/images/DL_basic/Untitled%2028.png)

- 컨볼루션 필터가 여러개 있을 수도 있음. 단, 한 번 컨볼루션을 거치고 나면 그 다음에는 non-linear activation 이 들어가게 됨. 즉, 입력 이미지(32*32*3)가 컨볼루션 필터(4 5*5*3)를 거쳐서 나온 컨볼루션 피쳐맵(28*28*4)의 각 element 별로 activation func 혹은 non-linear activation 을 거침(ReLU 등).

![Untitled](/assets/images/DL_basic/Untitled%2029.png)

- 컨볼루션 필터의 채널은 내가 지금 컨볼루션을 하는 feature(인풋 이미지)의 dimension이 됨.
- 항상 집중해서 봐야 할 것은 이 연산을 정의하는데 필요한 파라미터의 숫자를 잘 생각해야 함.
- 컨볼루션 커널의 size * 인풋 이미지의 채널 * 아웃풋 채널 숫자
- 일반적인 CNN 은 컨볼루션 레이어, pooling layer, fully connected layer 등으로 이루어져 있음.
- Convolution and pooling layers : feature extraction. 이미지에서 유용한 정보를 추출
- Fully connected layer : decision making. 분류를 하거나 회귀를 해서 내가 원하는 출력값을 얻어주는 것.
- 이렇게 만들어진 것이 일반적인 CNN. 최근 들어서 뒷단의 FC 가 점점 최소화시키는 추세. 왜냐면 파라미터 숫자에 dependent 하게 됨.
- 내가 학습하고자 하는 어떤 모델의 파라미터의 숫자. 내가 학습해야 하는 파라미터 숫자가 늘어날 수록 학습이 어렵고 일반화 성능이 떨어진다고 알려져 있음. 즉 내가 아무리 학습을 잘 시켜도 이 모델을 배포했을 때 성능이 안나오는 것.

→ CNN 이 발전하는 방향은 같은 모델을 만들고 최대한 모델을 deep 하게 가져가지만 동시에 파라미터 숫자를 줄이는데 집중하게 됨. 이를 위한 테크닉들이 많음. 계속 강조해야 하는 것은, 어떤 뉴럴 네트워크 아키텍쳐를 봤을 때 네트워크의 레이어 별로 몇 개의 파라미터로 이루어져 있고, 전체 파라미터 숫자가 몇 개 인지를 항상 몇 개 인지 보는 것. 그 감을 가지는 것이 중요하게 됨.

- 어떤 뉴럴 네트워크에 대해서, 파라미터가 몇 개인지를 손으로 계산해 보는 것이 좋음.
- patch size/stride, output size, params
- Stride 란?
- 넓게 걷는다 라는 것. stride 가 1이라는 것은 커널(컨볼루션 필터)을 매 픽셀마다 찍고 바로 한 픽셀 옮겨서 찍는 것. stride가 2이면 필터를 하나 찍고 바로 옆으로 옮기는 것이 아니라 2칸 옮기는 것. 내가 컨볼루션 필터를 얼마나 dense 혹은 sparse 하게 찍을 것이냐를 말하게 되는 것.

![이 그림에서는 1D Convolution](/assets/images/DL_basic/Untitled%2030.png)

이 그림에서는 1D Convolution

- 2D 로 가면 stride 의 파라미터가 2개가 됨. width, height 방향으로.
- Padding 이란?
- 일반적인 컨볼루션 을 돌리면 바운더리 정보가 버려짐. 내가 가진 이미지의 가장 가장자리를 찍을 수 없음. 컨볼루션 필터가 삐져나옴. 그래서 가장자리를 새로 만들어주는 것이 padding. 일반적으로 0-padding 은 그 덧대는 값이 0인 것.
- 즉 패딩을 해주면 인풋과 아웃풋의 dimension 이 똑같게 됨.

![Untitled](/assets/images/DL_basic/Untitled%2031.png)

- 적절한 크기의 패딩과 stride 가 1이면 어떤 입력과 convolution operator 의 출력으로 나오는 컨볼루션 피쳐맵의 dimesion 이 같아지는 것을 알 수 있음.
- 3*3 일 때는 패딩이 1, 5*5 일 때는 패딩이 2, 7*7 일 때는 패딩이 3이 필요함.
`kernel size // 2` → 적절한 패딩과 stride 가 1이면 항상 입력과 출력이 같다!
- 컨볼루션 연산을 할 때 내가 원하는 출력값에 맞춰서 제로 패딩과 stride 를 줄 수 있음.

![Untitled](/assets/images/DL_basic/Untitled%2032.png)

- 파라미터 숫자 계산.

<aside>
💡 먼저, Convolution 연산 후 Output image 의 크기(텐서 크기)는
(input size + (2*padding)- filter size)/stride + 1 이다.

![Untitled](/assets/images/DL_basic/Untitled%2033.png)

Pooling 연산 후 output image 의 크기(텐서 크기)는

![$I$ : input size, $P_s$ : Pooling size, $S$ = stride of convolution operation](/assets/images/DL_basic/Untitled%2034.png)

$I$ : input size, $P_s$ : Pooling size, $S$ = stride of convolution operation

</aside>

![Untitled](/assets/images/DL_basic/Untitled%2035.png)

- 컨볼루션 피쳐맵(연산으로 나오는 것)의 하나의 채널(한 겹)을 만들기 위해서는, 내가 컨볼루션 하고 싶은 커널의 special dimension(위에서는 3*3) 이 있고. 자동으로 각각의 커널의 채널 크기는 내가 가지고 있는 인풋 dim과 똑같게 됨. 즉 하나의 컨볼루션 필터(커널)의 채널은 인풋 이미지와 같음.
- 위 예에서 커널의 special dimension 이 3*3 이고, 인풋 이미지의 special dimension 이 40*50 일 때, 두 개의 채널 수는 같고, padding 이 1, stride 가 1 이면 입력이미지와 같은 special dimension 의 아웃풋 즉 컨볼루션 피쳐가 1겹 나오게 됨(즉 채널이 1). 그러나 우리가 궁극적으로 원하는 아웃풋의 채널은 64 이기 때문에, 컨볼루션 필터(커널)가 64개 필요한 것. 따라서 필요한 파라미터 숫자는 아래와 같이 된다.

$$
3 \times 3 \times 128 \times 64 = 73,728
$$

- padding 이나 stride 는 파라미터 숫자와는 무관함. CNN 에서 하나의 레이어의 파라미터 숫자는 오로지 내가 convolution operation 하는 필터 혹은 커널이 몇 개 있는지에 dependent 하기 때문. 뒤에가서 이런 계산이 자동으로 되어야 함. 네트워크의 파라미터 숫자를 네트워크 모양만 봐도 감이 생겨야 함. 이게 10000단위 인지 등 단위 개념에서 감이 생겨야 한다는 것.

→ **input, output 의 special dimension 은 중요치 않음. 이것은 파라미터 숫자와 상관없음. 왜냐면 conv 는 각각의 special 위치에 가해지는 convolution 필터가 동일하기 때문.**

- AlexNet 을 보자.

![Untitled](/assets/images/DL_basic/Untitled%2036.png)

→ AlexNet 은 그 당시 GPU 메모리 부족으로 모델을 2갈래로 나눠서 진행함. 그래서 마지막에 2가 곱해지는 것. 즉 첫번째 레이어에서 96 채널의 컨볼루션 피쳐맵을 만들어야 하는데 1개의 GPU 메모리 사이즈에 맞추다 보니 48로 한 것.

→ 중요한 것은, 여기서 보는 파라미터 숫자는 컨볼루션 단게에서는 많아야 884k 임. 그런데 뒷단의 fully connected layer 에 들어가면 달라짐.

![Untitled](/assets/images/DL_basic/Untitled%2037.png)

→ 여기서 부터는 Dense layer 임. fully connected layer 의 차원은 인풋의 파라미터 개수(뉴런의 개수) 와 아웃풋의 뉴런의 개수를 곱한 것 만큼이다. 값이 엄청 커지는 것.

→ 빨간색 layer 가 컨볼루션 layer, 파란색 layer 가 Dense layer(MLP) 인데 파라미터 숫자가 컨볼루션에 비해서 dense layer에 넘어가면서 거의 1,000배가 늘어남.

→ 왜 이럴까? 네트워크가 인풋에서 아웃풋으로 갈 때 크기가 엄청 변화되는 것도 아닌데, dense layer 가 일반적으로 훨씬 많은 파라미터를 갖게 되는 이유는, 컨볼루션 연산이 각각의 하나의 커널이 모든 위치에 대해서 동일하게 적용되기 때문. 즉, 컨볼루션 연산은 일종의 shared parameter. 같은 커널이 이미지의 오른쪽, 왼쪽, 위, 아래 모두 동일하게 적용됨.

→ 뉴럴 네트워크를 성능을 올리기 위해서는 파라미터를 줄이는 것이 중요. 따라서 대부분의 파라미터가 fc layer 에 들어가면 파라미터 수가 엄청 늘어나기 때문에, 네트워크가 발전되는 성향이 뒷단에 있는 fc layer 를 최대한 줄이고 앞단의 convolution layer를 최대한 깊게 쌓는 것이 일반적인 트렌드.

→ 여기에 추가로 들어가는 것이 1*1 convolution. 이러한 시도들로 뉴럴 네트워크의 깊이는 깊어지지만 파라미터 숫자는 줄어들고 성능은 올라감.

- 이를 위해 기본이 되는 게 1 x 1 convolution
- 1 x 1 conv는 , 이미지에서 영역을 보지 않음. 한 픽셀만 보는 것. 즉 채널 방향을 줄이는 것.
- 왜 할까? Dimension reduction. 1x1 conv 에서의 dimension 은 채널을 말함.
- 이것의 의미는, **컨볼루션 레이어를 깊게 쌓으면서 동시에 파라미터 숫자를 줄일 수 있게 됨. 즉 뎁스는 늘리고 파라미터 수는 줄임.**
- bottleneck architecture 가 바로 이 1x1 conv 을 이용해서 네트워크를 깊게 쌓는데 파라미터 숫자를 줄일 수 있는 것.

![Untitled](/assets/images/DL_basic/Untitled%2038.png)

- 실제 이 1x1 conv 테크닉은 굉장히 자주 사용됨. ResNet, DenseNet 등 다 활용됨.
- 나중에 컨볼루션 레이어의 채널 디멘션과 MLP의 히든 디멘션을 정해줄 수 있게 코딩하면 네트워크 튜닝할 때 편해짐.

- Modern CNNs
- AlexNet, ResNet, GoogLeNet, AlexNet, VGGNet, DenseNet
- ISVRC 에서 해마다 1등했던 모델들. 여전히 주의할 것은 각각 네트워크의 파라미터 숫자, 네트워크의  뎁스를 봐야함. → 네트워크의 뎁스는 점점 깊어지고, 파라미터는 줄어갈 것이고, 성능은 올라감.
- 네트워크를 깊게 쌓으면서 어떻게 파라미터 수를 줄일 수 있는지를 관심있게 보자.
- AlexNet(2012)
- 입력은 같지만 네트워크가 2개로 나뉨. GPU 메모리가 당시 부족했기 때문. GPU 를 최대한 활용하는데 네트워크에 많은 파라미터를 집어넣고 싶어서 이런 전략을 취한 것.
- 인풋 이미지에 11 x 11 커널을 사용함. 파라미터 숫자 관점에서 11x11 은 좋은 선택이 아님. 11x11 필터를 사용하면 receptive field 즉 하나의 커널이 볼 수 있는 이미지 영역은 커지지만 상대적으로 더 많은 파라미터가 필요함.

![Untitled](/assets/images/DL_basic/Untitled%2039.png)

- 8개의 레이어로 되어있음. 당시에는 딥 했음.
- Key ideas.
- ReLU 를 사용. 효과적인 activation func 임. activation func 이 가져야 하는 첫번째 가장 큰 성질은 non-linear. ReLU는 0을 기준으로 linear 한 부분도 있지만 전체 func 은 non-linear 함. 또한 0보다 클 때 기울기도 1이라서 기울기 소실(레이어를 깊게 쌓았을 때)도 잘 없음.
- 선형 모델이 가지는 좋은 성질을 가지고 있음. gradient 가 activation 값이 커도 gradient 를 그대로 가질 수 있음.
- 또한 선형 모델이 가지는 좋은 성능을 가지고 있기 때문에 gradient descent 기법들을 사용해서 학습이 용이함.
- 중요한 것은 기울기 소실 문제를 극복함.
- 이전에 활용하던 시그모이드, 하이퍼볼릭 탄젠트는 0을 기준으로 값이 커지면 slope(gradient) 가 줄어들게 됨. 내가 가진 뉴런의 값이 0에서 많이 벗어나게 되면, activation 을 기준으로 하는 gradient slope 는 0에 가깝게 됨. 그래서 기울기 소실이 생김.
- 2개의 GPU 사용
- local response normalization(LRN), overlapping pooling
- LRN 은 어떤 입력 공간에서 response 가 많이 나오는 몇 개를 죽이는 것. 최종적으로 원하는 것은 sparse 한 activation 이 나오길 원하는 것. 지금은 많이 사용되지 않음.
- Data augmentation
- Dropout

→ 지금 보면 당연한 것들… 2012년도에는 당연하지 않았음. 일반적으로 제일 잘되는 기준을 잡았던 것.

- VGGNet(2015)

![Untitled](/assets/images/DL_basic/Untitled%2040.png)

- **3x3 convolution 필터를 사용.**
- 1x1 컨볼루션을 fc layer 를 위해 사용함. 그런데 여기서는 파라미터 수를 줄이기 위해 사용된 것은 아님.
- Dropout 을 사용함.
- layer 수에 따라 VGG16, VGG19
- **왜 3x3 convolution?**
- **컨볼루션 필터의 크기를 생각해보면, 크기가 커지면서 가지는 이점은 하나의 필터에서 고려되는 인풋의 크기가 커진다는 것. 그것이 Receptive field 를 말함. 하나의 컨볼루션 피쳐맵 값을 얻기 위해서 고려할 수 있는 입력의 spatial dimension 이 바로 receptive field.**
- 3x3 conv 를 2번 했을 때, 가장 마지막 단의 하나의 값은, 중간 피쳐맵의 3x3 을 보게 되고,  그 중간 피쳐맵의 하나의 값은 인풋의 3x3 을 보는 것. 사실상 마지막 레이어의 하나의 값은 인풋 레이어의 5x5 의 픽셀값이 합쳐진 값이 됨. 그래서 3x3 이 두 번 이뤄지면 receptive field 는 5x5가 됨.
- 즉, **5x5 하나 사용하는 것과 3x3을 두 개 사용하는 것이 receptive field 측면에서 똑같음.**
그러나 총 파라미터 수가 달라짐.

![Untitled](/assets/images/DL_basic/Untitled%2041.png)

- 왜 이런 일이 일어날까?

→ 레이어를 두 개 쌓고, 그러니까 파라미터가 두 배가 늘어나서 3x3 conv 를 2개 쓴 게 더 많은 수가 있을 것 같지만, 사실상 3x3 은 곱해봤자 9. 2배 해봤자 18. 5x5 는 25.

→ **따라서 같은 receptive field 를 얻는 관점에서는 5x5 1개 보다 3x3을 2개 활용하는게 파라미터 수를 줄일 수 있음.**

→ 따라서 뒤에 나오는 대부분의 논문들에서 conv 필터(커널) 의 special dimension 은 7x7 을 벗어나지 않음. 커봤자 5x5 이다. 비슷한 이유로.

- GoogLeNet(2015)

![Untitled](/assets/images/DL_basic/Untitled%2042.png)

- 1x1 conv 은 dimension reduction 의 효과가 있고 여기서 말하는 dimension 은 channel. 그래서 컨볼루션 피쳐맵이 special dimension(W & H) 이 있는데 그게 아니라 그 이미지 tensor의 depth 방향에 해당하는 channel 을 줄이는 것.
- 이것은 네트워크를 똑같이 딥하게 쌓아도 중간에 1x1 conv 를 잘 활용하면 파라미터 숫자를 줄일 수 있게 됨.
- AlexNet → VGGNet 에서는 11x11, 5x5, 7x7 보다는 input 단의 special receptive field 를 늘리는 차원에서는 3x3 을 여러번 활용하는 것이 좋다는 것을 알게 됨.
- GoogLeNet 에서는 1x1 conv 를 잘 활용해서 전체적인 파라미터 수를 줄일 수 있는지를 볼 수 있음.
- 비슷하게 보이는 네트워크가 반복됨. 네트워크 모양이 네트워크 안에 있다고 해서 NIN(Network In Network) 구조라고 함.

![Untitled](/assets/images/DL_basic/Untitled%2043.png)

- Inception block? 하나의 입력이 들어왔을 때 퍼졌다가 합쳐짐. 중요한 것은, 각각의 path 를 보면 3x3 conv 를 하기 전에 1x1 Conv 가 들어감. 즉 Convolution filter 전에 1x1 이 들어가는 것. 이게 중요한 역할을 함.
- 왜 중요할까? 하나의 입력에 대해서 여러개의 receptive field 를 갖는 필터를 거치고 이것을 통해서 여러개의 repsonse 들을 concatenation 하는 효과도 있지만, 여기서 1x1 conv 이 중간에 있음으로서 전체 네트워크의 파라미터 수를 줄임.
- 어떻게 파라미터 숫자를 줄이는 것일까? → 그러면 이 1x1 conv 이 왜 중요한지를 알 수 있음.

→ 1x1 conv can be seen as channel-wise dimension reduction ; 채널 방향으로 줄인다!


![Untitled](/assets/images/DL_basic/Untitled%2044.png)

- 앞에서 5x5 보다 3x3이 더 파라미터 수를 줄일 수 있었음. 결과적으로 3x3 이 우리가 줄일 수 있는 최소한인데, 여기서 1x1 을 통해서 더 파라미터 수를 줄이는 것.
- 중간에 1x1 conv 을 통해서 special dimesion 은 그대로 가고 채널 방향으로 정보를 줄인 것. 이렇게 줄이고 얻어지는 ‘어떤 줄어든 채널을 가진 special dimension 의 conv feature map’에 3x3 conv 를 하는 것. 이렇게 만들어진 2layer 의 파라미터 숫자는 더 줄어듦.
- 그래서 입력과 출력만 봤을 때 채널은 똑같음. 또한 receptive field 도 같음. 3x3 를 사용했기 때문. 그런데 파라미터 숫자가 많이 줄어듦. 이런 1x1 conv 을 통해 채널 방향으로 dimension reduction 이 들어가면서 전체적인 파라미터 숫자는 줄어듦. 또한 입력과 출력만 놓고 보면 receptive field 는 똑같고 입력과 출력의 채널 또한 같음.

![Untitled](/assets/images/DL_basic/Untitled%2045.png)

- GoogLeNet 은 VGGNet 에 비해서 파라미터 수가 엄청 줄어듦. 네트워크는 3단 깊어짐. 이게 바로 뒷단의 dense layer를 줄이고, 11x11 를 줄이고 1x1 conv 로 feature dimension 을 줄이기 때문. → 즉 네트워크는 점점 깊어지고 파라미터 수는 줄어들고 성능은 좋아짐!!
- ResNet (2015)
- Generalization performance - train error 가 줄어듦에도 불구하고 test error 와 차이가 많이 나는 정도를 의미 → 차이가 적어야 일반화 성능이 좋다!
- 파라미터 수가 많은 것은 1) 오버피팅 문제가 있음. 2)
- 오버피팅은 학습 에러가 줄어드는데 테스트 에러가 커지는 것. 슬로프가 반대되는 것. 테스트 에러가 점점 커지는 것. 아래 그림은 오버피팅은 아님. 둘 다 같이 줄어들었기 때문.

![Untitled](/assets/images/DL_basic/Untitled%2046.png)

- 문제는 학습 에러가 작음에도 불구하고 테스트 에러가 더 큰 것. 즉 학습이 안되는 것.
56 레이어를 아무리 잘 학습시켜봤자 20레이어가 더 학습이 잘되는 것.
→ 오버피팅은 아니지만 네트워크가 커짐에 따라서 학습을 못 시키는 것.
- 그래서 ResNet 은 residual connection(identity map 혹은 skip connection)을 추가함.

![Untitled](/assets/images/DL_basic/Untitled%2047.png)

- skip connection 은 입력이 올라가면 출력이 나오는데, 그 입력을 뉴럴 네트워크의 출력 값 혹은 한단 짜리 conv layer 에 더해주는 것.
→ 궁극적으로 원하는 것은 이 conv layer 가 학습하고자 하는 것은 residual. 차이만 학습하는 것. 왜냐면 x에다가 f(x)를 더했기 때문. 실제로 f(x)가 학습하는 것은 그 차이를 학습하길 원하는 것. 그렇게 되면 아래처럼 깊어졌을 때 학습이 잘 될 수 있음.

⇒ **skip connection이 gradient 소실 문제 해결에도 도움이 되는데, Skip Connection은 그래디언트가 직접적으로 뒤로 전달될 수 있도록 하여 그래디언트 소실 문제를 완화한다.**

![Untitled](/assets/images/DL_basic/Untitled%2048.png)

- identity map 을 사용하면 **학습 자체**를 더 잘 시킬 수 있게 되는 것. 네트워크를 딥하게 쌓을 수 있는 가능성을 열어준 것. 깊게 쌓아도 학습 가능!

![Untitled](/assets/images/DL_basic/Untitled%2049.png)

- 중요한 점은, x 를 더하려면 차원이 같아야 함. 내가 입력이 128x128x64 일 때, 3x3conv - BN - ReLU - 3x3conv - BN 을 통과하고 나온 값이 역시 128x128x64 가 되어야 함.
- 차원을 맞춰주기 위해서 1x1 conv 로 채널을 바꿔주는 것이 projected Shortcut
- 재밌는 것은, BN 이 컨볼루션 뒤에 일어나게 됨. 그 다음에 activation 이 일어남.
- BN 을 ReLU 다음에 넣어야 잘된다, 안넣어야 잘된다 이런 말들도 있음. 원래 논문에서는 BN 이 Conv 다음에 나옴.

![Untitled](/assets/images/DL_basic/Untitled%2050.png)

- 여기도 Bottleneck arch 가 나옴. 3x3 conv 를 위해서는 파라미터 수는 3 x 3 x input_channel x output_channel. 내가 3x3 conv 하기 전에 input_channel 을 줄이면 전체적인 파라미터 수를 줄일 수 있음. 그래서 **3x3 conv 전에 input 채널을 줄이고, 3x3 conv 이후에 input 채널을 늘리는 용도로 1x1 conv 가 사용됨**. 그래서 1x1 conv 이 2번 들어가는 것. 이것을 통해서 내가 궁극적으로 원하는 output conv feature map 의 채널의 차원을 맞출 수 있는 것.

⇒ 정리하면, 1x1 conv 를 이용해서 채널을 줄이게 되고, 그렇게 줄어든 채널에서 3x3 conv 을 함으로써 receptive field 를 키우고, 다시 1x1 conv 로 원하는 채널을 맞춰주는 테크닉. 이런 식으로 파라미터 숫자는 줄임과 동시에 네트워크를 깊게 쌓아서 receptive field 를 키우는 게 전략.

- DenseNet (2017)
- 아이디어가 간단함. ResNet 을 high-level view 에서 바라보게 되면 conv을 통해서 나오는 값을 그냥 더하는 것(인풋을). 여기서 두 개가 섞이니까 더하지 말고, special dimension 이 같으니까 두 개를 concatenation 하자! 는 것.

![Untitled](/assets/images/DL_basic/Untitled%2051.png)

- concatenate 할 때 문제는 채널이 점점 커짐. 채널이 기하급수적으로 커짐. 왜냐하면 뒤에 있는 것은 앞에 있는 모든 채널을 다 concatenate 했기 때문(값을 더하는 게 아닌 붙히는 것!).

![Untitled](/assets/images/DL_basic/Untitled%2052.png)

- 그래서 채널이 커지면, 거기서 가해지는 conv feature map 의 채널도 같이 커짐. 따라서 파라미터 수도 같이 커짐. 그래서 우리가 원하는 것은, 중간에 한번씩 채널을 줄여야 함. 즉 파라미터 수를 줄여야 함. → 여기서 1x1 conv 가 사용됨.
- Dense Block
- feature map 을 계속 키우는 것(concate)
- Transition Block
- BN → 1x1 conv → 2x2 AvgPooling
- 1x1 conv 에서 conv feature map size 를 확 줄임. 다시 Dense Block 으로 쭉 늘임. 그리고 다시 Transition Block 으로 쭉 줄임. 계속 반복하는 것이 DenseNet

![Untitled](/assets/images/DL_basic/Untitled%2053.png)

⇒ receptive field 를 늘리는 입장에서는 VGG

⇒ GoogLeNet 에서 1x1 conv 로 채널 수를 줄여서 파라미터 수를 줄임.

⇒ ResNet 에서 skip connection 으로 네트워크를 깊게 쌓을 수 있게 함.

⇒ DenseNet에서는 concatenation 으로 피쳐맵을 더하는 것 대신에 쌓으면서 더 좋은 성능을 낼 수 있었다는 것.