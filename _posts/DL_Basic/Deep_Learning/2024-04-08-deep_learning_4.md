---
title: "[Deep Learning] 4. Activation Function"
excerpt: "활성화 함수(Activation Function) 에 대해 알아보자"

categories: "deep_learning"
tags:
    - deep learning
    - Activation
toc: true  
toc_sticky: true
toc_label: "Contents In Page"
author_profile: true
use_math: true

date: 2024-04-08
---

## 활성화 함수(Activation Function)란?
- 입력 신호의 총합을 출력 신호로 변환하는 함수다. 이를 통해 이전 층(layer)의 결과값을 변환하여 다른 층의 뉴런으로 전달할 수 있다. 대표적으로 Sigmoid, Softmax, ReLU 등이 있다.
- NN을 살펴봤던 [포스트](https://bkkhyunn.github.io/deep_learning/deep_learning_0-1/)에서 공부했듯, 인공 신경망은 인간 두뇌 활동을 모방하기 위해 뉴런의 구조를 참고했다. 최초의 인공 신경망인 퍼셉트론은 **계단 함수**를 활성화 함수로 사용했다.
- 그러나 계단 함수를 활성화함수로 사용하면, 인공 신경망의 학습이 제대로 이루어지기 어려웠다.
  - 인간은 하나의 분야를 탐구하면서 지속적으로 학습하는데, 계단 함수는 이러한 인간 학습의 연속성을 표현할 수 없었기 때문이다.
  - 인공 신경망은 학습을 위하여 **경사하강법**을 이용하는데, 이는 **연쇄법칙**을 활용한 **미분**을 토대로 계산된다. 그러나 계단함수는 미분 불가능한 함수이기 때문에 **경사하강법을 통한 역전파**로 학습이 되지 않는다.
- 따라서 값을 전달할 때 연속성을 부여할 수 있는, 즉 **미분 가능한 함수**를 사용하고자 했다.
- 활성화 함수의 또 다른 특징은 **비선형 함수**를 사용한다는 것이다.
- 그 이유의 첫번째는 신경망의 표현성을 높이기 위해서이다. 현실 세계의 대부분 문제는 비선형 문제이기 때문에, 비선형 활성 함수를 사용하면 모델이 문제를 더 효과적으로 풀 수 있게 된다. 또한, 다양한 비선형 활성 함수를 조합하여 사용함으로써 그 효과는 배가 될 수 있다.
- 두번째는 선형 함수로는 독립변수와 종속변수 사이의 다양한 관계를 파악하기 어렵다. 그리고 인공 신경망은 성능을 위하여 은닉층을 여러 층으로 쌓는데, 선형 함수가 여러개 쌓이는 것은 결국 선형함수이다. 즉, 활성 함수로 선형 함수를 사용하게 되면, 여러 층을 통과하는 결과값을 단 하나의 층으로로 표현할 수 있게 되면서 신경망을 깊게 쌓는 의미가 희석되는 것이다.
- 수식적으로 살펴보면, 선형 함수는 $f(x) = \mathbf{W}\mathbf{x} + \mathbf{b}$ 로 나타낼 수 있고 이는 입력에 따라서 출력이 상수배만큼 변함을 의미한다. 즉 여러 개의 선형 함수는 결과 만큼의 상수배를 이용하여 하나의 활성 함수로도 충분히 표현이 가능하다. 이렇게 여러개의 은닉층을 가진 신경망과 1개의 은닉층 가진 신경망의 차이가 거의 없어지게 되는 것이다.
- 모델 입장에서 봤을 때, 이는 **모델의 표현성과 성능을 현격히 저하**시키고 **층을 깊게 쌓는 의미를 퇴색**시킨다.
- 정리하면, 활성화 함수는 (1)**미분 가능한**, (2)**비선형** 함수를 이용한다.
- 이제 아래의 그림의 나와있는 것처럼 다양한 활성화 함수의 종류를 살펴보자.
![Untitled](/assets/images/DL_basic/activate.png){: .align-center}

## Sigmoid

## Softmax

## ReLU